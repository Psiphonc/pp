# 摘要
对话式推荐系统（CRS）能够通过在一次对话中多次询问用户属性和推荐物品与用户交互，以明确获取用户对物品和属性的偏好，从而弥补了传统推荐系统的不足。现有的对话式推荐算法普遍采用基于 DQN （#CITE [28]）的强化学习算法进行策略学习。由于 DQN 在推荐系统的大规模动作空间上的效率问题，先前的大多工作对动作空间进行了筛选，这一定程度上限制了对话策略的推荐能力。此外，现有的对话式推荐策略很少考虑用户负反馈信息，这严重损害了模型对用户偏好的建模能力。 为了解决这些问题，我们提出了一种名AHPCR (Adaptive Hybird Policy-based Conversational Recommender)的方法，将对话式推荐问题抽象为混合动作空间上的决策问题。AHPCR 采用分层 Actor 网络来决策动作类型和动作参数，并在动作空间上搜索与参数网络给出的原动作（Action Prototype）相似度最高的动作作为最终决策。此外，我们还提出了一种基于用户反馈的 MDP 环境建模方法，将用户负反馈信息分别建模为正反馈图和负反馈图。实验结果显示，AHPCR在四个基准数据集上的推荐性能显著优于基线方法。
# 介绍
传统的推荐系统通常使用隐式反馈，例如用户点击记录和好友点击记录来学习用户的兴趣偏好。然而，在在线系统中，用户的反馈信息往往未得到充分利用。对话式推荐系统通过与用户进行对话来了解和认识用户的兴趣偏好，从而获得用户的显式反馈。这种技术架起了搜索和传统推荐系统之间的桥梁，假设用户在一定程度上了解自身需求，通过多轮对话逐步揭示用户的兴趣偏好，并向用户推荐可能感兴趣的物品。此技术对电子商务、社交媒体等行业具有重要意义，成为网络平台和学者热议的主题。
目前，在对话式推荐系统领域存在两个不同的研究方向。基于属性的对话式推荐系统致力于构建策略模块，通过尽可能少的对话次数实现尽可能准确的推荐。而生成式对话式推荐系统则更注重流畅的对话体验，并通过将推荐物品相关的信息灵活地融入回复文本中，提高推荐结果的可解释性。本文研究了多回合对话推荐（Multi-round Conversational Recommendation，MCR），即对一个用户进行多次推荐并根据之前推荐的反馈来改善后续的推荐，以在最少的对话次数内实现更准确的推荐效果。
对话式推荐算法可分为两类：一类采用独立的对话策略和推荐模块，另一类将对话策略和推荐统一为在由项目和属性构成的动作空间中选择动作的策略学习过程。前者采用离线和在线两阶段训练过程，但训练过程中对话策略和推荐模块相互影响不足，导致策略学习难以收敛。后者将物品和相关属性视为动作空间，并使用DQN评估动作价值，然而在大规模动作空间中，由于效率问题，无法对大量动作逐一评估，常需对动作空间中的动作先行粗筛，可能排除最佳动作。
同时，现有的基于对话的推荐模型通常使用基于图的MDP（#CITE [19]）环境来建模当前对话状态。根据用户反馈在图上修剪候选项目和属性，并通过图卷积等深度学习方法提取对话状态特征。然而，这些方法没有充分利用负面用户反馈中的隐式信息。目前，用户的负面反馈信息仅用于修剪用户表示图，导致当前对话状态的建模能力不足。用户的负面反馈信息包含用户的偏好信息。例如，当用户拒绝一个项目时，可以推断出该项目包含用户不喜欢的属性。此外，使用单个图对当前对话状态进行建模可能会丢失全局对话信息。
为了解决以往工作中的不足，本研究将对话式推荐问题抽象为一个混合动作空间决策问题。我们使用分层的Actor网络分别对类型（属性查询或项目推荐）和参数（具体属性或项目）进行决策，计算动作空间中候选动作与参数网络生成的原动作之间的相似度，选择相似度最高的动作作为决策结果。此外，为了有效利用用户反馈中隐含的用户偏好信息，我们基于用户的正反馈和负反馈分别构建两个反馈图，并使用独立的图神经网络（）提取用户反馈中的高阶特征，以提高模型的对话状态建模性能和决策效果。
综上所述，本文的主要贡献如下：

- 我们提出将对话推荐问题建模为混合动作空间上的决策问题，这使得策略网络可以直接生成决策结果而非在庞大的动作空间上进行价值评估，规避了动作空间筛选对决策效果的限制；
- 我们提出了一种基于用户反馈的对话状态建模方法，通过引入用户负反馈图，提高了对当前对话状态建模的能力；
- 在4个公共基准数据集上的实验结果表明，该方法的性能明显优于目前最先进的 CRS 方法。
# 相关工作
## CRS的相关工作 
对话式推荐系统（CRS）是一种集解释性和准确性于一身的推荐模式，它通过主动询问用户以获得用户的显式反馈。目前，CRS研究可分为以下四个方向（#CITE [1] [2] ）：（1）“探索-利用权衡”(#CITE [3] [4] [5])是处理冷启动问题的有效手段，它利用bandit方法在对话推荐场景中平衡冷启动用户的探索和利用。（2）基于问题的用户偏好诱导（#CITE [6] [7] [8]）则着重于构建问题以获取尽可能多的信息，并利用用户反馈来提供更适当的推荐。（3）多轮对话推荐（#CITE [9] [10] [11]）则主要侧重于与用户反复互动，以动态适应用户的反应，并在尽可能少的对话轮次中成功地推荐。（4）对话理解与生成（#CITE [12] [13] [14] [15]）则致力于从用户的话语中理解他们的偏好和意图，并生成自然、流畅的自然语言与用户沟通。在这些问题中，我们关注的是多轮对话推荐问题（MCR）。
在CRS系统与用户交互的每个轮次中，都存在两个决策过程：何时做出推荐和具体询问或推荐哪个属性或项目。早期的MCR工作主要改进了询问时机和询问哪个属性的策略，而具体推荐何物的决策则由外部的启发式规则来完成。例如EAR模型（#CITE [9]）以 FM 作为外部的启发式推荐规则来决策具体推荐什么项目。并使用基于策略梯度的强化学习算法来优化询问属性的时机和所询问的属性的选择策略。为了减少策略学习的操作空间，SCPR（#CITE [10]）将策略优化为只决策询问或推荐，并将对话推荐场景建模为图上的交互式路径推理问题。然而，这些工作隔离了推荐和对话策略，使策略学习难以收敛。为了解决这个问题，Unicorn 模型（#CITE [16]）将两个决策过程统一为从由项目和属性组成的候选动作空间中进行选择的过程。HICR（#CITE [23]）进一步在统一策略中考虑相似用户的层次关系，并使用序列模型建模用户对话的顺序关系。
## 强化学习推荐系统的相关工作
强化学习由于其可以捕获用户动态兴趣的特点（#CITE [26] [27]），在推荐系统领域得到了广泛的应用。目前，根据其深度强化学习算法的不同，强化学习推荐系统（RLRS）的研究可分为两类：（1）基于价值的方法通过学习值函数来评估每个动作的价值，借此来确定在每个状态下应该采取的最佳动作，这类算法利用贝尔曼方程来更新值函数。然而，当处理推荐系统中大量项目构成的离散动作空间时，基于价值的方法存在性能问题（#CITE [20]）。（2）基于策略梯度的方法直接显式地学习一个目标策略，并通过策略梯度来更新策略以最大化累积回报的期望值（#CITE [21]）。此类方法训练出来的智能体在泛化能力上有所欠缺，无法对没有训练过的动作进行决策（#CITE [22]）。
在CRS领域，早期的研究工作都采用基于价值的强化学习方法进行策略学习。为了避免在大规模的动作空间上评估动作价值的性能问题，这些研究在评估动作价值之前采用了动作选择策略。例如，在 UNIRCON 和 HICR 中，使用图上结点的嵌入来计算项目得分，同时利用与属性相关的项目权重计算属性的熵，进而对动作空间中的属性和项目进行筛选。动作筛选的准确性很大程度上限制了策略决策的准确性。在最坏情况下，最优动作可能从一开始就被从候选动作中筛选出去了。在我们的工作中，将CRS问题创新地建模为混合动作空间（#CITE [24]）上的决策任务。我们将决策任务分为两步：决策动作类型和动作参数。动作类型是指在由询问属性和推荐项目两个动作构成的动作空间上进行的离散决策任务。而动作参数则是将具体的项目或属性视为向量空间上的点的连续决策任务。同时，在我们的方法采用了基于PPO（#CITE [25]）算法来优化策略网络。
# 问题定义
本文关注多轮对话推荐（Multi-Round Conversational Recommendation，MCR）场景。在该场景下，CRS 通过在有限的对话轮次中询问属性或推荐项目，来向用户推荐目标项目。在 CRS 中，维护了一组要推荐的物品$\mathcal{V}=\{v_1, v_2, \cdots, v_M\}$，对于每个物品$v_i$都与一组属性$\mathcal{P}_{v_i}$相关联。每一轮对话从用户开始，用户$u$通过指定一个喜欢的属性$p_0$来初始化。然后，CRS可以询问用户关于候选属性集$\mathcal{P}_{cand}$中的属性，或从候选物品集合$\mathcal{V}_{cand}$中推荐一定数量的物品。用户会根据个人偏好做出响应，接受或拒绝提出的属性或推荐的物品。随后，CRS 会根据用户的反馈更新候选属性和物品集，同时决定下一步行动。对话将持续进行，直到CRS进行了成功的推荐或达到最大轮数$T$。
我们将上述用户与 CRS 交互的过程描述为尔科夫决策过程（MDP）。CRS 的目标是学习一个策略$\pi$，以最大化观察到的 MCR 对话中累积回报的期望值，即
$\pi^*=\argmax_{\pi\in\Pi}\mathbb{E}[\sum^T_{t=0}r(s_t,a_t)]$，
其中$s_t$是从系统状态和对话历史中学习得到的状态表示，$a_t$是智能体在𝑡时刻采取的动作，$r(\cdot)$是内在奖励函数，有时也简写为$r_t$。
# 方法
本文提出的方法 AHPCR 总体架构如下图所示，由 Graph Based MDP Environment，Graph Encoder 和 Agent 三个部分组成。
![AHPCR 的总体架构图](https://cdn.nlark.com/yuque/0/2023/png/2506274/1686837231368-075b3058-ce67-4fcf-80dc-2a477f00ee79.png#averageHue=%23f7f2e8&clientId=u04e9931e-f758-4&from=paste&height=4590&id=ude9cda2b&originHeight=4590&originWidth=6700&originalType=binary&ratio=1&rotation=0&showTitle=true&size=2688476&status=done&style=none&taskId=ua7eab5d4-b43d-47b2-88a6-adf072b325c&title=AHPCR%20%E7%9A%84%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE&width=6700 "AHPCR 的总体架构图")
## Graph Based MDP Environment
现有的对话式推荐系统(CRS)通常将推荐过程抽象为用户与推荐智能体之间交互的马尔可夫决策过程(MDP)。在在我们的研究中，同样用 MDP 来建模推荐问题。我们定义 MDP 环境，其中包括状态空间$\mathcal{S}$，动作空间$\mathcal{A}$，状态转移函数$\mathcal{T}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}$和奖励函数$\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow R$，构成一个四元组$(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R})$。
### State
在基于图的MDP环境中，状态应该包含用户与系统交互的状态信息。我们将对话状态$s_t$定义为包含t时刻前的对话历史和t时刻下时用户相关的反馈图的二元组：
$s_t=(\mathcal{H}^{(t)}_u, \mathcal{G}_u^{(t)} )$,
其中对话历史$\mathcal{H}^{(t)}_u=(\mathcal{P}_{acc}^{(t)},\mathcal{P}_{rej}^{(t)},\mathcal{V}_{rej}^{(t)})$包含用户接受的属性$\mathcal{P}_{acc}^{(t)}$，用户拒绝的属性$\mathcal{P}_{rej}^{(t)}$和用户拒绝的项目$\mathcal{V}_{rej}^{(t)}$。用户反馈图$\mathcal{G}_u^{(t)}=(\mathcal{G}_u^{(t)+}, \mathcal{G}_u^{(t)-})$包含了用户正反馈图$\mathcal{G}_u^{(t)+}$和用户负反馈图$\mathcal{G}_u^{(t)-}$。它们的构建方式将在后续的 Graph Encoder 章节中详述。
### Action
推荐智能体根据当前状态$s_t$在动作空间$\mathcal{A}$中选择最优动作$a_t$执行，其中动作空间$\mathcal{A}=\{\mathcal{V_{cand}^{(t)}},\mathcal{P}^{(t)}_{cand}\}$由候选项目集$\mathcal{V_{cand}^{(t)}}$和候选属性集$\mathcal{P}^{(t)}_{cand}$构成，它们分别具有如下定义：
$\mathcal{V_{cand}^{(t)}}=\mathcal{V}_{\mathcal{P}_{acc}^{(t)}}\backslash\mathcal{V}_{rej}^{(t)}$，
$\mathcal{P}_{cand}^{(t)}=\mathcal{P}_{\mathcal{V}_{cand}^{(t)}}\backslash(\mathcal{P_{acc}^{(t)}\cup\mathcal{P_{rej}^{(t)}}})$。
其中，候选项目集由所有与用户接受属性相关联的未交互过的项目构成，候选属性集则由所有与候选项目相关联的未交互过的属性构成。当智能体决定询问属性时，会从候选属性集$\mathcal{P}^{(t)}_{cand}$中选择一个属性询问用户；当决定推荐项目时，会从候选项目集$\mathcal{V_{cand}^{(t)}}$中选择一定数量的项目进行推荐。
### Transition
状态转移函数$\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ 根据时刻$t$的状态$s_t$和智能体从动作空间$\mathcal{A}$中选择的动作$a_t$给出下一个状态$s_{t+1}$。动作空间包含询问属性$p_t$ 和推荐项目$i_t$两类动作。若用户接受请求询问的属性$p_t$，则下一个状态$s_{t+1}$更新为：$\mathcal{P}^{(t+1)}_{acc} = \mathcal{P}^{(t)}_{acc} \cup \{p_t\}$。否则，若用户拒绝了询问的属性，则下一个状态$s_{t+1}$更新为：$\mathcal{P}^{(t+1)}_{rej}=\mathcal{P}^{(t)}_{rej} \cup \{ p_t \}$。此外，如果用户拒绝了推荐的产品$i_t$，则将该项目加入到对话历史的拒绝项目集合中：$\mathcal{V}^{(t+1)}_{rej}=\mathcal{V}^{(t)}_{rej} \cup \{i_t\}$。否则，若用户接受了推荐的产品$i_t$，则本轮推荐结束。
### Reward
本文遵循之前关于MCR研究（#CITE [10] [16]）提出的设定，采用包括五种奖励类型的奖励函数。首先，当用户接受推荐的物品时，会产生非常大的正奖励$𝑟_{rec\_suc}$；其次，当用户拒绝推荐的物品时，环境则给予负奖励$𝑟_{rec\_fail}$；当用户接受了系统属性的询问时会给予一个较小的正奖励$𝑟_{ask\_suc}$；当用户拒绝了系统属性的询问时，环境将给予负奖励$𝑟_{ask\_fail}$。最后，当智能体达到最大轮数时，环境会给予较大的负奖励$𝑟_{quit}$来惩罚智能体。
## Graph Encoder
在基于图的 MDP 环境中，我们将用户、项目和属性表示为异构图中的节点。我们采用 TRANS-E （#CITE [31]）进行预训练，确保异构图中的节点表示具备结构化特征。Graph Encoder 首先按照一定的规则构建用户的正、负反馈图，然后通过图卷积神经网络（GCN）对两个反馈图进行了图卷积，有效地捕获节点间的高阶连通性。最后，将正反馈图和负反馈图上的用户节点表示拼接并添加信息位，从而得到当前对话的状态表示。
### 用户反馈图构建
我们采用用户正反馈图和用户负反馈图来分别建模正反馈和负反馈相关的节点，以有效地呈现当前对话历史中用户、项目和属性之间的关系。图结构用二元组$\mathcal{G} = (\mathcal{N}, A)$表示，其中$\mathcal{N}$表示节点集合，$A$是邻接矩阵。用户正反馈图包含以下两种边，分别是用户与用户接受的属性之间的边和用户接受属性与候选项目之间的边。形式上，正反馈图$\mathcal{G}^{(t)+}$的节点集合$\mathcal{N}^{+(t)}$定义为$\{u\}\cup\mathcal{P}_{\text{acc}}^+\cup\mathcal{P}_{\text{cand}}^{(t)}\cup\mathcal{V}_{\text{cand}}^{(t)}$，邻接矩阵的定义如下：
$A_{i, j}^{+(t)}= 
\begin{cases}
1, & \text { if } n_i=u, n_j \in \mathcal{P}_{\text{acc}}^+ \\ 
1, & \text { if } n_i \in \mathcal{P}_{\text{acc}}^+ , n_j \in \mathcal{V}_{cand}\\ 
0, & \text { otherwise }
\end{cases}$。
用户负反馈图包括表示被拒绝属性和项目的节点，以及对话历史中的候选属性和项目节点。负反馈图$\mathcal{G}^{(t)-}$的节点集合$\mathcal{N}^{-\,(t)}$的形式化定义为$\{u
\}\cup\mathcal{P}_{\text{rej}}^{(t)}\cup\mathcal{V}_{\text{rej}}^{(t)}\cup\mathcal{P}_{\text{cand}}^{(t)}\cup\mathcal{V}_{\text{cand}}^{(t)}$，邻接矩阵的定义如下：
$A_{i, j}^{-(t)}= 
\begin{cases}
1, & \text { if } n_i = u,  n_j \in \mathcal{V}_{rej} \\ 
1, & \text { if } n_i \in \mathcal{V}_{rej}, n_j \in \mathcal{P}_{cand} \\ 
1, & \text { if } n_i = u, n_j \in \mathcal{P}_{rej} \\
1, & \text { if } n_i \in \mathcal{P}_{rej}, n_j \in \mathcal{V}_{\text {cand }} \\ 0, & \text { otherwise }\end{cases}$。
### 图表示学习
我们采用图神经网络（GNN）从用户的正反馈和负反馈图中提取用户、项目和属性之间的高阶结构信息。具体而言，我们使用图卷积网络（GCN）在图上进行多层消息传递，$e_i^{(l+1)}$表示GCN的第$l+1$层节点$n_i$，其计算方法如下：
$e_i^{(l+1)}=\operatorname{ReLU}\left(\sum_{j \in \mathcal{N}_i} \Lambda_{i, j} W_l e_j^{(l)}+B_l e_i^{(l)}\right)$。
其中，$\mathcal{N}_i$表示节点$n_i$的相邻节点集合，$W_l$ 和$B_l$是可训练参数。此外，$\Lambda$是一个归一化的邻接矩阵，其计算公式为：
$\Lambda=D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$，
其中$D$是一个对角矩阵，其对角元素计算为
$D_{i i}=\sum_j A_{i, j}$。
经过$L$层图卷积后，用户节点聚合了图上所有连通节点的特征。因此，我们使用用户节点来代表用户的正反馈（$e_u^+$）和负反馈（$e_u^-$）。最后，我们将两者进行拼接并在末尾拼接上与对话状态相关的三个信息位。$e_{turn}$表示当前对话的进度，即$e_{turn}=\frac{current\_turn}{max\_turn}$；$e_{item}$表示当前环境中剩余的候选项目的比例，即 $e_{item}=\frac{cand\_item}{total\_item}$；$e_{attr}$表示当前环境中剩余的属性的比例，即$e_{attr}=\frac{attr\_num}{total\_attr}$。最终的对话状态表示向量表示为：
$\hat e_s = e_u^+ \oplus e_u^- \oplus [e_{turn}, e_{item\_num},e_{attr\_num}]$。
## Agent
本文将 CRS 环境中的候选属性和候选项目建模为包含属性和项目两类动作的混合动作空间，其中每个动作对应一组参数，代表特定属性或项目。为了确保嵌入向量在空间中具有一定的结构化特征，我们使用了 TRANS-E 算法对用户、项目和属性组成的异构图进行预训练，获得节点的嵌入向量。接着，我们遵循了H-PPO的做法（#CITE [32]），设计了分层 Actor 网络来决策动作类型和动作参数。最后，我们使用PPO算法对策略网络进行优化。
### 混合动作空间
先前的研究将所有的属性和项目视作离散动作，然而，在实际的工业应用中，CRS可能拥有数百万个项目和属性。动作空间的大小会线性增加决策时间复杂性，将它们视为离散动作将导致计算效率低下（#CITE [33]）。为了解决这个问题，AHPCR 将 CRS 环境中的大规模离散动作空间建模为混合动作空间。具体而言，考虑一个可参数化的混合动作空间。离散动作空间是一个由询问属性和推荐项目组成的二元集合$A_d=\{a_{ask}, a_{rec}\}$，每个动作都对应一个连续参数集$\mathcal{X}_a \subseteq R^{m_a}$，其中$m_a$是 CRS 中属性和项目经过预训练后得到的特征向量的维度。我们将 CRS 智能体在混合动作空间上生成的动作称为动作原型$a=(\tau, x)$，其中$\tau\in A_d$是离散动作类型，$x\in\mathcal{X}_a$是一个连续动作参数。每个离散动作及其参数空间联合构成整个混合动作空间：
$\mathcal{A}=\bigcup_{\tau\in A_d}\{(\tau, x)|x\in\mathcal{X}_a\}$。
在智能体给出动作原型$a$后，环境根据动作类型$\tau$在选择在候属性集合$\mathcal{V}_{cand}$或候选项目$\mathcal{P}_{cand}$中进行相似度搜索，以获得最终的动作$\hat{a}$：
$\hat a =  \underset{d\in\mathcal{D_{cand}(\tau)}}{\text{argmax}}\ x \cdot e_{d}$，
其中，$e_d$表示预训练的嵌入表中查询到的动作$d$的嵌入向量，$\mathcal{D}_{cand}(t)$是根据动作类型$\tau$选择的候选动作集合：
$\mathcal{D}_{cand}(\tau)= 
\begin{cases}
\mathcal{V}_{cand}, & \text { if } \tau=a_{ask} \\ 
\mathcal{P}_{cand}, &  \text { if } \tau=a_{rec}\\ 
\end{cases}$。
### Hybrid Actor-Crtic
通常，Actor-Critic 算法包括 Actor 网络和 Critic 网络。Actor 学习策略，Critic 估计价值函数并通过价值函数指导 Actor 网络的学习。AHPCR 采用了 H-PPO 提出的 Hybrid Actor-Critic 体系结构来处理混合动作空间。针对 CRS 问题，我们使用了两层 Actor 网络架构，包括一个离散 Actor 网络来学习动作类型策略 $\pi_{\theta_d}$，以及两个独立的 Actor 网络$\pi_{\theta_\text{attr}}$和$\pi_{\theta_\text{item}}$分别学习项目和属性的连续参数策略。前者选择离散动作空间$A_d$中的动作类型$\tau$，后者使用前者的输出来遮盖后两种参数策略的输出，最终将两者配对形成一个完整的动作原型$a=(\tau,x)$。具体来说，三个Actor网络都是以上一章所述的 Graph Encoder 输出的状态表示向量$\hat e_{s}$作为输入的多层感知机。区别在于，第一层的动作类型 Actor 网络输出两个动作类型对应的logit值，记为$z \in \mathbb{R}^{2}$，它们经过softmax函数得到离散策略$\pi_{\theta_\text{type}}$选择每个动作的概率分布：
$\pi_{\theta_d}(\tau=i|s)=\frac{\exp(z_i)}{\sum_{j=1}^{2}\exp(z_j)}$。
而第二层的参数 Actor 网络在连续参数空间采用参数化分布的形式，针对连续动作的每一维输出相应的参数$\mu$和$\Sigma$，它们作为动作概率分布的均值和方差构建一个高斯分布，并借助这个高斯分布来进行后续的动作选择，即：
$x|s\sim\mathcal{N}(\mu(s),\Sigma(s))$。
同时，我们假设不同维度的动作参数是相互独立的分布，因此协方差矩阵可以退化成对角矩阵$[\sigma_{ii}]$，离散Actor网络定义为：
$\pi_{\theta_c\in{\{\theta_{attr},\theta_{item}\}}}(x \mid s)=\frac{\exp \left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right)}{\sqrt{(2 \pi)^k|\Sigma|^{-1}}}$。
### Hybrid Proximal Policy Optimization
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2506274/1682312002999-2a739575-ee53-4994-8d62-8c1c6bc2d558.png#averageHue=%23f0f0f0&clientId=u64317ed7-4a88-4&from=paste&height=494&id=u6cf1cfc6&originHeight=1180&originWidth=1386&originalType=binary&ratio=1&rotation=0&showTitle=false&size=485727&status=done&style=none&taskId=u2b9f8bf4-5c01-4528-890c-e0a4fef202a&title=&width=580)
在 MCR 过程中，CRS智能体需要与环境进行$T$步交互。每一步交互，智能体使用 Graph Encoder 对当前状态$s_t$进行编码，然后根据编码后的状态表示向量$s_t$生成动作原型$a_t=(\tau_t,x_t)$。接下来，智能体使用动作原型选择候选集合，并进行内积相似度搜索以得到动作$\hat a_t$。智能体在环境中执行该动作，并获得回报$r_t$，同时将状态从$s_t$转换为$s_{t+1}$。我们将$T$步交互产生的数据按照时间顺序排列，组成训练数据轨迹(Trajectory)，记作：
$\text{traj}=(<s_0, a_0, r_0>,<s_1, a_1, r_1>,\cdots,<s_T, a_T, r_T>)$。

为了提高PPO训练的稳定性，我们采用了广义优势估计（GAE）对状态价值函数进行估计。具体来说，我们根据数据轨迹中的记录逐一计算GAE优势函数$\hat A_t$和目标价值函数$\hat R_t$：
$\hat{A_t} = \sum_{i=t}^{T-1} (\gamma \lambda)^{i-t} \delta_i$，
$\hat R_t=\hat A_t + V(s_t)$。
其中，T代表轨迹的长度，$\gamma$是折扣因子，$\lambda$是一个控制偏差-方差权衡的超参数，$\delta_i = r_i + \gamma V_{\phi_{old}}(s_{i+1}) - V_{\phi_{old}}(s_i)$是时刻$i$的TD误差，$V_{\phi_{old}}$是基于Critic网络的状态价值函数估计。
我们采用近端策略优化（PPO），这是一种先进的策略优化方法。PPO通过最小化一个剪裁代替目标函数$L^{\text{clip}}(\theta)$来学习策略$\pi_\theta$：
$L^{clip}(\theta) = \mathbb{E}_t \left[ \min \left(r_t(\theta) \hat{A_t}, clip \left(r_t(\theta), 1-\epsilon, 1+\epsilon \right) \hat{A_t} \right) \right]$
其中$r_t(\theta)$表示一个概率比例$\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$，$\epsilon$是一个超参数。在训练期间，离散策略$\pi_{\theta_d}$和连续策略$\pi_{\theta_c}$分别通过最小化其各自的剪裁代替目标函数进行更新。离散策略$\pi(\theta^d)$的目标由$L^{clip}(\theta^d)$给出，而连续策略$\pi(\theta^c)$的目标由$L^{clip}(\theta^c)$给出。两者的概率比例$r_t^d$和$r_t^c$分别定义为$\frac{\pi_{\theta^d}(a_t|s_t)}{\pi_{\theta^d_{old}}(a_t|s_t)}$和$\frac{\pi_{\theta^c}(a_t|s_t)}{\pi_{\theta^c_{old}}(a_t|s_t)}$。虽然这两种策略在决策动作原型时共同工作，但在优化时$\pi_{\theta^d}$和$\pi_{\theta^c}$被视为两个独立分布进行策略优化，而不是联合分布。此外，我们采用最小化均方差损失来更新Critic网络：
$L_\phi=\frac{1}{2}\sum_t(V_\phi(\hat e_{s_t} )-\hat R_t)^2$。
最后，我们构建了联合损失函数：
$L= \eta L_\phi - \alpha L^{clip}(\theta^d)-\beta L^{clip}(\theta^c)$。
其中$\alpha$、$\beta$和$\eta$均为调整联合损失权重的超参数。
# 实验
## 实验设置
### 数据集
我们在四个已有的多回合会话推荐基准数据集上评估了 AHPCR，这些数据集的统计数据如下表所示：

|               | LastFM  | LastFM* | Yelp      | Yelp*     |
| ------------- | ------- | ------- | --------- | --------- |
| #Users        | 1,801   | 1,801   | 27,675    | 27,675    |
| #Items        | 7,432   | 7,432   | 70,311    | 70,311    |
| #Interactions | 76,693  | 76,693  | 1,368,606 | 1,368,606 |
| #Attributes   | 33      | 8,438   | 29        | 590       |
| #Entities     | 9,266   | 17,671  | 98,605    | 98,576    |
| #Relations    | 4       | 4       | 3         | 3         |
| #Triplets     | 138,215 | 228,217 | 2,884,567 | 2,533,827 |

- LastFM 和 Yelp：LastFM 数据集用于评估音乐艺术家推荐，而 Yelp 数据集用于商业推荐。为了便于建模，Lei 等人（#CITE [9]）人工将 LastFM 的原始属性合并成33个粗粒度组，并为 Yelp 构建了一个二级分类，其中一级分类有29个。
- LastFM* 和 Yelp*：Lei 等人（#CITE [10]）认为手动合并属性在实际应用中并非最佳实践，因此他们使用原始的属性重构了上述两个数据集。为了公平比较，我们在实验中同时使用了两个版本。
### 指标
我们采用了对话推荐的三个广泛使用的指标： SR@T， AT和 hDCG 。其中，SR@T（成功率）用于衡量t轮内的累计推荐成功率；AT（平均回合数）用于评估所有对话的平均轮次数；hDCG@(T, K) 用于评价推荐的排名性能。 hDCG@(T, K) 将 NDCG 扩展到两层的对话式推荐系统中，其中 K 和 K 分别代表推荐列表的长度和推荐轮次，其计算方法如下：
$h D C G @(T, K)=  \sum_t^T \sum_k^K r(t, k)\left[\frac{1}{\log _2(t+2)}\right. \left.+\left(\frac{1}{\log _2(t+1)}-\frac{1}{\log _2(t+2)}\right) \frac{1}{\log _2(k+1)}\right]$。
### 实现细节
我们将最大对话轮次数T设为15，将推荐列表的大小𝐾设为10，并将每个数据集分割为$7:1.5:1.5$用于训练、验证和测试。我们采用OpenKE（#CITE [35]）中实现的 TransE 算法在基于训练集构建的图上进行节点表示的预训练。我们使用用户模拟器（#CITE [9] [35]）与CRS交互，使用验证集在线训练模型。对于所有上述实现的方法，我们都进行了50000轮对话的在线训练。我们采用与先前工作相同的奖励设置对模型进行训练（#CITE [9] [10] [16]）：$r_{rec\_suc}=1$，$𝑟_{rec\_fail}=-0.1$，$𝑟_{ask\_suc}=0.01$，$𝑟_{ask\_fail}=-0.1$，$𝑟_{quit}=-0.3$。超参数的经验设置如下：嵌入大小和隐藏层大小分别设置为64和100。GCN层数$L_g$，Transformer层数$L_s$都设置为2。在PPO的训练过程中，每次迭代收集的轨迹数量为2048条，mini-batch的大小为258，更新10次。学习率和$L_2$范数正则化设置为 1e-4 和1e-5，使用Adam优化器。折扣因子$\gamma$和GAE的$\lambda$分别设置为0.9和0.95。动作类型网络、动作参数网络和状态价值网络的clip系数分别设置为0.2、0.2和0.5。
## 基线方法
为验证AHPCR的有效性，我们选取了几种最先进的方法进行比较。具体如下：

- Max Entropy：此方法采用基于规则的策略进h h h行询问和推荐，根据当前状态选择熵值最大的属性，或以一定概率推荐高排名的项目。
- Abs Greedy：此方法同样采用基于规则的策略进行询问和推荐，只进行物品推荐操作，并根据反馈信息更新模型，会持续推荐项目，直至成功推荐或达到预定的一轮。
- CRM：一种基于强化学习的方法，它将用户的偏好记录到信念跟踪器中，并根据信念跟踪器学习决定何时询问以及询问哪些属性的策略。
- EAR：该方法引入了一个三阶段的解决方案，以增强会话组件和推荐组件之间的交互性。
- SCPR：该方法将CRS建模为交互式路径推理问题，根据用户反馈遍历图上的属性顶点，以排除无关的候选属性。
- UNICORN：这是一种基于强化学习的方法，统一两种决策策略。它通过图神经网络学习RL的图增强状态表示。
- MCMIPL：此方法将统一的会话推荐策略扩展为用户的多兴趣表示。
- HICR：这是一种先进的CRS方法，利用相似用户连接的分层关系以及一次对话中的多轮对话历史来增强对话状态的表示。
## 性能比较
| 
 | LastFM      |           |           | LastFM*   |           |           | Yelp      |           |          | Yelp*     |           |           |
 | ----------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | -------- | --------- | --------- | --------- | --------- |
 |             | SR@15     | AT        | hDCG      | SR@15     | AT        | hDCG      | SR@15     | AT       | hDCG      | SR@15     | AT        | hDCG      |
 | Abs Greedy  | 0.222     | 13.48     | 0.073     | 0.635     | 8.66      | 0.267     | 0.264     | 12.57    | 0.145     | 0.189     | 13.43     | 0.089     |
 | Max Entropy | 0.283     | 13.91     | 0.083     | 0.669     | 9.33      | 0.269     | 0.921     | 6.59     | 0.338     | 0.398     | 13.42     | 0.121     |
 | CRM         | 0.325     | 13.75     | 0.092     | 0.580     | 10.79     | 0.224     | 0.923     | 6.25     | 0.353     | 0.177     | 13.69     | 0.070     |
 | EAR         | 0.429     | 12.88     | 0.136     | 0.595     | 10.51     | 0.230     | 0.967     | 5.74     | 0.378     | 0.182     | 13.63     | 0.079     |
 | SCPR        | 0.465     | 12.86     | 0.139     | 0.709     | 8.43      | 0.317     | 0.973     | 5.67     | 0.382     | 0.489     | 12.62     | 0.159     |
 | UNICORN     | 0.535     | 11.82     | 0.175     | 0.788     | 7.58      | 0.349     | 0.985     | 5.33     | 0.397     | 0.520     | 11.31     | 0.203     |
 | MCMIPL      | 0.633     | 11.54     | 0.191     | 0.839     | 6.89      | 0.412     | 0.981     | 5.65     | 0.387     | 0.552     | 11.31     | 0.178     |
 | HICR        | 0.773     | 9.97      | 0.240     | 0.885     | 6.06      | 0.427     | 0.983     | **4.90** | **0.423** | 0.549     | 11.73     | 0.175     |
 | AHPCR       | **0.834** | **9.581** | **0.259** | **0.941** | **5.352** | **0.471** | **0.993** | 5.285    | 0.387     | **0.678** | **11.20** | **0.205** |

从上表中报告的所有方法的总体性能来看，我们得出以下结论:

- **AHPCR在四个数据集的推荐成功率指标上都有显著提升。**尤其是在动作空间最大的Yelp*数据集上，AHPCR在推荐成功率上比次优基线方法提升了12.9%。改进的原因有以下几个方面：(i) AHPCR将MCR问题视为混合动作空间上的参数决策问题，这使得策略网络可以直接生成决策结果而非在庞大的动作空间上进行价值评估，规避了动作空间筛选对决策效果的限制；(ii)  AHPCR基于用户反馈模对话状态，有效捕获了用户负反馈中包含的用户偏好信息，提高了状态建模能力。
- **AHPCR在平均推荐轮次数指标上有所提升，且在三个数据集上取得最优。**但在Yelp数据集上的表现为次优。实际上，在Yelp数据集的实验中，AHPCR在第13轮时的推荐成功率已经达到0.987，超过所有基线方法。同时，我们推测为了获取更高的推荐正确率，AHPCR可能倾向于采取更为保守的推荐策略，以获得更高的奖励。
- **两个决策过程间的相互影响对CRS的表现有重要的影响。**从表中可以观察到，AHPCR、HICR、MCMIPL和UNICORN在四个数据集的三个指标上均优于CRM、EAR和SCPR。这主要有两个原因：(i) CRM、EAR和SCPR在训练过程中采用基于强化学习的对话策略和基于外部启发式规则的推荐模块，这两者的分离导致了对话推荐策略难以收敛。(ii) AHPCR、HICR、MCMIPL和UNICORN采用强化学习的方法来学习用户对属性和物品的动态偏好。
## Study of AHPCR
### 不同轮次数下AHPCR的推荐性能
上图展示了每轮的成功率（SR@𝑡），从图中可以看出，AHPCR在不同数据集上展示出了各异的推荐策略。实验过程中，我们发现在属性数量最多的LastFM*数据集上，许多与属性相关的项目数量本身就少于10个。因此，AHPCR在LastFM上采取了较激进的策略，选择在首轮便尝试进行推荐。然而，在其他属性数量较少的数据集中，AHPCR的推荐策略相对保守，特别是在LastFM数据集中，我们的方法在前几轮几乎未进行推荐。这得益于我们的方法将MCR问题视作混合动作空间上的参数决策问题，使用独立的网络进行属性询问或项目推荐的决策。
![不同轮次数下AHPCR的推荐成功率](https://cdn.nlark.com/yuque/0/2023/png/2506274/1688093478816-aec95d2c-fe99-4f85-8a79-2c36e500b431.png#averageHue=%23fdfdfc&clientId=ua7292115-7d30-4&from=paste&height=262&id=ufc830e91&originHeight=775&originWidth=1163&originalType=binary&ratio=1&rotation=0&showTitle=true&size=166925&status=done&style=none&taskId=u6bf221d1-fb95-4f7a-b6e8-5b82ed2321f&title=%E4%B8%8D%E5%90%8C%E8%BD%AE%E6%AC%A1%E6%95%B0%E4%B8%8BAHPCR%E7%9A%84%E6%8E%A8%E8%8D%90%E6%88%90%E5%8A%9F%E7%8E%87&width=393 "不同轮次数下AHPCR的推荐成功率")
### 消融实验
| 
 | LastFM             |           |          | LastFM*   |           |          | Yelp      |           |          | Yelp*     |           |           |
 | ------------------ | --------- | -------- | --------- | --------- | -------- | --------- | --------- | -------- | --------- | --------- | --------- | --------- |
 |                    | SR@15     | AT       | hDCG      | SR@15     | AT       | hDCG      | SR@15     | AT       | hDCG      | SR@15     | AT        | hDCG      |
 | AHPCR              | **0.834** | **9.58** | **0.259** | **0.941** | **5.35** | **0.471** | **0.993** | **5.28** | **0.387** | **0.678** | **11.20** | **0.205** |
 | w/o Negative       |
 | Feedback           | 0.827     | 9.68     | 0.244     | 0.918     | 5.71     | 0.367     | 0.985     | 5.94     | 0.339     | 0.518     | 12.28     | 0.147     |
 | w/o                |
 | Type Head          | 0.830     | 9.70     | 0.256     | 0.851     | 7.21     | 0.335     | 0.993     | 5.35     | 0.387     | 0.649     | 11.52     | 0.195     |
 | w/o                |
 | Pretrain Embedding | 0.738     | 11.28    | 0.214     | 0.651     | 7.89     | 0.372     | 0.925     | 7.98     | 0.325     | 0.135     | 14.18     | 0.045     |

我们的方法 AHPCR 的核心设计包括以下三点：(i) 我们用户正反馈图的基础上引入了用户负反馈图；(ii) 我们使用分层Actor网络决策动作类型和参数；(iii) 我们将MCR系统中的项目和属性建模为混合动作空间上的连续参数。为了证实这些设计的有效性，我们在AHPCR的基础上分别去掉负反馈图编码器、类型Actor网络，以及采用随机初始化的嵌入向量表示MCR中的属性和项目。从上表结果可以看出： 

- **在大规模动作空间上，编码用户负反馈信息十分重要。**在动作空间最大的数据集Yelp*上，移除用户负反馈编码器模块后，AHPCR的推荐性能显著下降。
- **采用混合参数空间建模AHPCR可以有效提升推荐性能。**在属性数量最多的数据集LastFM*上，移除分层Actor网络导致推荐性能显著下降。
- **异构图中的节点表示具备结构化特征对参数Actor网络的策略学习至关重要。** 用随机初始化的嵌入向量来表示MCR中的属性和项目，会使AHPCR在四个数据集上的推荐性能都显著下降，

下两图分别展示了AHPCR和其消融模型在训练过程中的Episodic Reward和对话轮次数。
![AHPCR与消融模型在训练过程中的Episodic Reward](https://cdn.nlark.com/yuque/0/2023/png/2506274/1687954163374-78581b57-093a-41d4-9e29-c5e7cbf9a054.png#averageHue=%23fefdfd&clientId=u0f25e034-bac1-4&from=paste&height=820&id=u8b665d63&originHeight=1640&originWidth=3024&originalType=binary&ratio=2&rotation=0&showTitle=true&size=845076&status=done&style=none&taskId=u2fdfaf66-b3ea-4027-9605-af2ac83d12f&title=AHPCR%E4%B8%8E%E6%B6%88%E8%9E%8D%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84Episodic%20Reward&width=1512 "AHPCR与消融模型在训练过程中的Episodic Reward")
![AHPCR与消融模型在训练过程中的Episode length](https://cdn.nlark.com/yuque/0/2023/png/2506274/1687954274595-1b8632bc-b15b-4cc8-b420-9f39ebc9f21a.png#averageHue=%23fefdfd&clientId=u0f25e034-bac1-4&from=paste&height=873&id=uc9078e53&originHeight=1746&originWidth=3248&originalType=binary&ratio=2&rotation=0&showTitle=true&size=1035538&status=done&style=none&taskId=u2940d5ab-3229-43cb-acbe-250f3001f7a&title=AHPCR%E4%B8%8E%E6%B6%88%E8%9E%8D%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84Episode%20length&width=1624 "AHPCR与消融模型在训练过程中的Episode length")
### 图神经网络的层数
下表展示了改变图神经网络层数实验的结果。实验发现，GNN层数的增加并不总是能显著提高AHCPR的推荐性能。相反，其在Yelp*数据集上的推荐性能反而有所下降。我们将这一现象归咎于邻居跳数增加带来的噪音。

| 
 | LastFM  |           |          | LastFM*   |           |          | Yelp      |           |          | Yelp*     |           |           |
 | ------- | --------- | -------- | --------- | --------- | -------- | --------- | --------- | -------- | --------- | --------- | --------- | --------- |
 |         | SR@15     | AT       | hDCG      | SR@15     | AT       | hDCG      | SR@15     | AT       | hDCG      | SR@15     | AT        | hDCG      |
 | AHPCR   |
 | (2 GNN) | 0.834     | 9.58     | 0.259     | 0.941     | 5.35     | 0.471     | 0.993     | 5.28     | 0.387     | **0.678** | **11.20** | **0.205** |
 | 3 GNN   | 0.843     | 9.42     | 0.263     | 0.944     | 5.28     | 0.472     | 0.993     | 5.32     | 0.383     | 0.6681    | 11.33     | 0.201     |
 | 4 GNN   | **0.849** | **9.41** | **0.265** | **0.959** | **5.14** | **0.474** | **0.994** | **5.32** | **0.385** | 0.676     | 11.34     | 0.203     |

下两图分别展示了不同层数GNN的AHPCR在训练过程中的Episodic Reward和对话轮次数。
![不同层数的图卷积的Episodic Reward](https://cdn.nlark.com/yuque/0/2023/png/2506274/1686832941582-2cd48485-3674-407c-b63e-704fd45bacaf.png#averageHue=%23fefcfb&clientId=u04e9931e-f758-4&from=paste&height=1286&id=oFBrs&originHeight=1286&originWidth=2396&originalType=binary&ratio=1&rotation=0&showTitle=true&size=866111&status=done&style=none&taskId=u040f2d85-b388-43ea-b83a-9ec1caeeaad&title=%E4%B8%8D%E5%90%8C%E5%B1%82%E6%95%B0%E7%9A%84%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%9A%84Episodic%20Reward&width=2396 "不同层数的图卷积的Episodic Reward")![不同层数的图卷积的Episode Length](https://cdn.nlark.com/yuque/0/2023/png/2506274/1686833073439-a087f969-7451-4758-bfb5-684f2a22b22c.png#averageHue=%23fdfbfb&clientId=u04e9931e-f758-4&from=paste&height=1272&id=sjcCM&originHeight=1272&originWidth=2380&originalType=binary&ratio=1&rotation=0&showTitle=true&size=870684&status=done&style=none&taskId=u03a289b5-599a-4167-9ba1-5440b5bff1d&title=%E4%B8%8D%E5%90%8C%E5%B1%82%E6%95%B0%E7%9A%84%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%9A%84Episode%20Length&width=2380 "不同层数的图卷积的Episode Length")
# 结论
在本研究中，我们提出了一种Adaptive Hybird Policy-based Conversational Recommender（AHPCR），将对话式推荐问题抽象为混合动作空间上的决策问题，通过分层的Hybrid Actor-Critic架构决策动作的类型和参数。同时，我们进一步通过用户负反馈编码器建模用户负反馈。实验结果表明，我们的方法在4个基准数据集上的推荐效果显著优于目前最先进的CRS方法。
